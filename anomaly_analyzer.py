#!/usr/bin/env python3
"""
Analiza anomalii CPU/Memory z uÅ¼yciem Qwen2:8b model
Zbiera metryki systemowe i opisuje anomalie przy pomocy LLM
"""

import ollama
import psutil
import time
from datetime import datetime
from typing import Dict, List
import json


class AnomalyAnalyzer:
    def __init__(self, model: str = "qwen3:8b"):
        """
        Inicjalizuj analizator anomalii
        
        Args:
            model: Nazwa modelu w Ollama (default: qwen3:8b)
        """
        self.model = model
        self.metrics_history: List[Dict] = []
        
    def collect_metrics(self) -> Dict:
        """Zbierz metryki CPU i pamiÄ™ci"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": psutil.virtual_memory().used / (1024**3),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "cpu_count": psutil.cpu_count(),
            "processes_count": len(psutil.pids())
        }
    
    def is_anomaly(self, metrics: Dict, cpu_threshold: float = 80, mem_threshold: float = 85) -> bool:
        """
        SprawdÅº czy metryki wskazujÄ… anomaliÄ™
        
        Args:
            metrics: Slownik z metrykami
            cpu_threshold: PrÃ³g dla CPU (%)
            mem_threshold: PrÃ³g dla pamiÄ™ci (%)
        
        Returns:
            True jeÅ›li anomalia, False w przeciwnym razie
        """
        return metrics["cpu_percent"] > cpu_threshold or metrics["memory_percent"] > mem_threshold
    
    def analyze_anomaly(self, metrics: Dict) -> str:
        """
        Przeanalizuj anomaliÄ™ przy pomocy LLM
        
        Args:
            metrics: Slownik z metrykami
            
        Returns:
            Opis anomalii od modelu
        """
        prompt = f"""Przeanalizuj te metryki systemowe i zidentyfikuj potencjalne anomalie:

CPU: {metrics['cpu_percent']:.1f}%
PamiÄ™Ä‡: {metrics['memory_percent']:.1f}% ({metrics['memory_used_gb']:.2f}GB / {metrics['memory_total_gb']:.2f}GB)
Liczba procesÃ³w: {metrics['processes_count']}
Liczba rdzeni CPU: {metrics['cpu_count']}
Czas: {metrics['timestamp']}

OdpowiedÅº powinna zawieraÄ‡:
1. Czy to jest anomalia? (Tak/Nie)
2. Jakie sÄ… przyczyny wysokiego zuÅ¼ycia?
3. Jakie dziaÅ‚ania podjÄ…Ä‡?
4. Czy grozi to problemami wydajnoÅ›ci?

BÄ…dÅº zwiÄ™zÅ‚y i konkretny."""

        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            stream=False
        )
        
        return response['response']
    
    def monitor_continuous(self, duration_seconds: int = 60, interval_seconds: int = 10):
        """
        CiÄ…gÅ‚e monitorowanie i analiza anomalii
        
        Args:
            duration_seconds: Jak dÅ‚ugo monitorowaÄ‡ (sekundy)
            interval_seconds: InterwaÅ‚ zbierania metryk (sekundy)
        """
        print(f"ğŸ” Rozpoczynam monitorowanie na {duration_seconds}s...")
        print(f"   Model: {self.model}")
        print(f"   InterwaÅ‚: {interval_seconds}s\n")
        
        start_time = time.time()
        anomaly_count = 0
        
        while time.time() - start_time < duration_seconds:
            # Zbierz metryki
            metrics = self.collect_metrics()
            self.metrics_history.append(metrics)
            
            print(f"[{metrics['timestamp']}]")
            print(f"  CPU: {metrics['cpu_percent']:.1f}% | Memory: {metrics['memory_percent']:.1f}%")
            
            # SprawdÅº anomaliÄ™
            if self.is_anomaly(metrics):
                anomaly_count += 1
                print("  âš ï¸  ANOMALIA WYKRYTA!")
                print("\nğŸ“Š Analiza LLM:")
                print("-" * 60)
                
                analysis = self.analyze_anomaly(metrics)
                print(analysis)
                
                print("-" * 60 + "\n")
            else:
                print("  âœ… Status normalny\n")
            
            time.sleep(interval_seconds)
        
        # Podsumowanie
        self.print_summary(anomaly_count)
    
    def print_summary(self, anomaly_count: int):
        """WyÅ›wietl podsumowanie sesji monitorowania"""
        print("\n" + "="*60)
        print("ğŸ“ˆ PODSUMOWANIE SESJI")
        print("="*60)
        
        if not self.metrics_history:
            print("Brak danych do analizy")
            return
        
        cpu_values = [m["cpu_percent"] for m in self.metrics_history]
        mem_values = [m["memory_percent"] for m in self.metrics_history]
        
        print(f"Liczba pomiarÃ³w: {len(self.metrics_history)}")
        print(f"Anomalie wykryte: {anomaly_count}")
        print(f"\nCPU:")
        print(f"  Åšrednia: {sum(cpu_values)/len(cpu_values):.1f}%")
        print(f"  Min: {min(cpu_values):.1f}% | Max: {max(cpu_values):.1f}%")
        print(f"\nPamiÄ™Ä‡:")
        print(f"  Åšrednia: {sum(mem_values)/len(mem_values):.1f}%")
        print(f"  Min: {min(mem_values):.1f}% | Max: {max(mem_values):.1f}%")


def main():
    """GÅ‚Ã³wna funkcja"""
    print("ğŸ¤– AIOps - Analiza Anomalii z Qwen2:8b\n")
    
    # SprawdÅº dostÄ™pnoÅ›Ä‡ Ollamy
    try:
        models = ollama.list()
        print(f"âœ… Ollama dostÄ™pna")
        print(f"   DostÄ™pne modele: {len(models['models'])}\n")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d: Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z Ollama: {e}")
        print("   Upewnij siÄ™, Å¼e Ollama jest uruchomiona: ollama run qwen2:8b")
        return
    
    # UtwÃ³rz analizator
    analyzer = AnomalyAnalyzer(model="qwen3:8b")
    
    # Monitoruj przez 60 sekund z interwaÅ‚em 10 sekund
    analyzer.monitor_continuous(duration_seconds=60, interval_seconds=10)


if __name__ == "__main__":
    main()
