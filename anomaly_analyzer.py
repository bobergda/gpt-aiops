#!/usr/bin/env python3
"""
Analiza anomalii CPU/Memory z u≈ºyciem Qwen2:8b model
Zbiera metryki systemowe i opisuje anomalie przy pomocy LLM
"""

import ollama
import psutil
import time
from datetime import datetime
from typing import Dict, List
import json


class AnomalyAnalyzer:
    def __init__(self, model: str = "qwen3:8b"):
        """
        Inicjalizuj analizator anomalii
        
        Args:
            model: Nazwa modelu w Ollama (default: qwen3:8b)
        """
        self.model = model
        self.metrics_history: List[Dict] = []
        
    def collect_metrics(self) -> Dict:
        """Zbierz metryki CPU i pamiƒôci"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": psutil.virtual_memory().used / (1024**3),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "cpu_count": psutil.cpu_count(),
            "processes_count": len(psutil.pids())
        }
    
    def is_anomaly(self, metrics: Dict, cpu_threshold: float = 80, mem_threshold: float = 85) -> bool:
        """
        Sprawd≈∫ czy metryki wskazujƒÖ anomaliƒô
        
        Args:
            metrics: Slownik z metrykami
            cpu_threshold: Pr√≥g dla CPU (%)
            mem_threshold: Pr√≥g dla pamiƒôci (%)
        
        Returns:
            True je≈õli anomalia, False w przeciwnym razie
        """
        return metrics["cpu_percent"] > cpu_threshold or metrics["memory_percent"] > mem_threshold
    
    def analyze_anomaly(self, metrics: Dict) -> Dict:
        """
        Przeanalizuj anomaliƒô przy pomocy LLM
        
        Args:
            metrics: Slownik z metrykami
            
        Returns:
            Odpowied≈∫ zwr√≥cona przez Ollamƒô
        """
        prompt = f"""Przeanalizuj te metryki systemowe i zidentyfikuj potencjalne anomalie:

CPU: {metrics['cpu_percent']:.1f}%
Pamiƒôƒá: {metrics['memory_percent']:.1f}% ({metrics['memory_used_gb']:.2f}GB / {metrics['memory_total_gb']:.2f}GB)
Liczba proces√≥w: {metrics['processes_count']}
Liczba rdzeni CPU: {metrics['cpu_count']}
Czas: {metrics['timestamp']}

Odpowied≈∫ powinna zawieraƒá:
1. Czy to jest anomalia? (Tak/Nie)
2. Jakie sƒÖ przyczyny wysokiego zu≈ºycia?
3. Jakie dzia≈Çania podjƒÖƒá?
4. Czy grozi to problemami wydajno≈õci?

BƒÖd≈∫ zwiƒôz≈Çy i konkretny."""

        return ollama.generate(
            model=self.model,
            prompt=prompt,
            stream=False
        )

    @staticmethod
    def print_llm_stats(response: Dict):
        """Wy≈õwietl statystyki u≈ºycia token√≥w i czasu"""
        prompt_tokens = response.get("prompt_eval_count")
        completion_tokens = response.get("eval_count")
        total_duration = response.get("total_duration")
        eval_duration = response.get("eval_duration")
        load_duration = response.get("load_duration")

        print("\nüìä Statystyki analizy:")
        if prompt_tokens is not None:
            print(f"  Tokeny promptu: {prompt_tokens}")
        if completion_tokens is not None:
            print(f"  Tokeny odpowiedzi: {completion_tokens}")
        if prompt_tokens is not None and completion_tokens is not None:
            print(f"  Tokeny ≈ÇƒÖcznie: {prompt_tokens + completion_tokens}")
        if total_duration:
            print(f"  Czas ca≈Çkowity: {total_duration / 1e9:.2f}s")
        if eval_duration:
            print(f"  Czas generowania: {eval_duration / 1e9:.2f}s")
        if load_duration:
            print(f"  Czas ≈Çadowania modelu: {load_duration / 1e9:.2f}s")
    
    def monitor_continuous(self, duration_seconds: int = 60, interval_seconds: int = 10):
        """
        CiƒÖg≈Çe monitorowanie i analiza anomalii
        
        Args:
            duration_seconds: Jak d≈Çugo monitorowaƒá (sekundy)
            interval_seconds: Interwa≈Ç zbierania metryk (sekundy)
        """
        print(f"üîç Rozpoczynam monitorowanie na {duration_seconds}s...")
        print(f"   Model: {self.model}")
        print(f"   Interwa≈Ç: {interval_seconds}s\n")
        
        start_time = time.time()
        anomaly_count = 0
        
        while time.time() - start_time < duration_seconds:
            # Zbierz metryki
            metrics = self.collect_metrics()
            self.metrics_history.append(metrics)
            
            print(f"[{metrics['timestamp']}]")
            print(f"  CPU: {metrics['cpu_percent']:.1f}% | Memory: {metrics['memory_percent']:.1f}%")
            
            # Sprawd≈∫ anomaliƒô
            if self.is_anomaly(metrics):
                anomaly_count += 1
                print("  ‚ö†Ô∏è  ANOMALIA WYKRYTA!")
                print("\nüìä Analiza LLM:")
                print("-" * 60)
                
                analysis_response = self.analyze_anomaly(metrics)
                print(analysis_response['response'])
                self.print_llm_stats(analysis_response)
                
                print("-" * 60 + "\n")
            else:
                print("  ‚úÖ Status normalny\n")
            
            time.sleep(interval_seconds)
        
        # Podsumowanie
        self.print_summary(anomaly_count)
    
    def print_summary(self, anomaly_count: int):
        """Wy≈õwietl podsumowanie sesji monitorowania"""
        print("\n" + "="*60)
        print("üìà PODSUMOWANIE SESJI")
        print("="*60)
        
        if not self.metrics_history:
            print("Brak danych do analizy")
            return
        
        cpu_values = [m["cpu_percent"] for m in self.metrics_history]
        mem_values = [m["memory_percent"] for m in self.metrics_history]
        
        print(f"Liczba pomiar√≥w: {len(self.metrics_history)}")
        print(f"Anomalie wykryte: {anomaly_count}")
        print(f"\nCPU:")
        print(f"  ≈örednia: {sum(cpu_values)/len(cpu_values):.1f}%")
        print(f"  Min: {min(cpu_values):.1f}% | Max: {max(cpu_values):.1f}%")
        print(f"\nPamiƒôƒá:")
        print(f"  ≈örednia: {sum(mem_values)/len(mem_values):.1f}%")
        print(f"  Min: {min(mem_values):.1f}% | Max: {max(mem_values):.1f}%")


def main():
    """G≈Ç√≥wna funkcja"""
    print("ü§ñ AIOps - Analiza Anomalii z Qwen2:8b\n")
    
    # Sprawd≈∫ dostƒôpno≈õƒá Ollamy
    try:
        models = ollama.list()
        print(f"‚úÖ Ollama dostƒôpna")
        print(f"   Dostƒôpne modele: {len(models['models'])}\n")
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd: Nie mo≈ºna po≈ÇƒÖczyƒá siƒô z Ollama: {e}")
        print("   Upewnij siƒô, ≈ºe Ollama jest uruchomiona: ollama run qwen2:8b")
        return
    
    # Utw√≥rz analizator
    analyzer = AnomalyAnalyzer(model="qwen3:8b")
    
    # Monitoruj przez 60 sekund z interwa≈Çem 10 sekund
    analyzer.monitor_continuous(duration_seconds=60, interval_seconds=10)


if __name__ == "__main__":
    main()
